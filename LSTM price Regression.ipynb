{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BigData & Blockchain Lab\n",
    "#### BTC-USD Price Prediction as regression problem(Buy/Sell) with Deep LSTM NN.\n",
    "\n",
    "\n",
    "#### Presented by @SAnbaee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, LSTM, BatchNormalization\n",
    "from matplotlib import pyplot as plt\n",
    "import ta\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SEQ_LEN = 7 # how long of a preceeding sequence to collect for RNN\n",
    "FUTURE_PERIOD_PREDICT = 1 # how far into the future are we trying to predict?\n",
    "RATIO_TO_PREDICT = \"BTC-USD\"\n",
    "EPOCHS = 30  # how many passes through our data\n",
    "BATCH_SIZE = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(df):\n",
    "    #df = df.drop(\"future\", 1)  # don't need this anymore.\n",
    "    df = df.drop(\"High\", 1)  # don't need this anymore.\n",
    "    df = df.drop(\"Low\", 1)  # don't need this anymore.\n",
    "    df = df.drop(\"Open\", 1)  # don't need this anymore.\n",
    "    \n",
    "    for col in df.columns:  # go through all of the columns\n",
    "        if col != \"target\":  # normalize all ... except for the target itself!\n",
    "            df[col] =[float(e) for e in df[col]]\n",
    "            df[col] = df[col].pct_change()  # pct changefor  \"normalizes\"\n",
    "            df = df.replace([np.inf, -np.inf], None)\n",
    "            df.dropna(inplace=True )  # remove the nas created by pct_change\n",
    "            df[col] = preprocessing.scale(df[col].values)  # scale between 0 and 1.\n",
    "    \n",
    "    df.dropna(inplace=True)  # cleanup again... jic.\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def preprocess_df(df):\n",
    "    \n",
    "    df = normalization(df)\n",
    "    sequential_data = []  # this is a list that will CONTAIN the sequences\n",
    "    \n",
    "    # These will be our actual sequences. \n",
    "    #they are made with deque, which keeps\n",
    "    #the maximum length by popping out older \n",
    "    #values as new ones come in\n",
    "    prev_days = deque(maxlen=SEQ_LEN)  \n",
    "\n",
    "    for i in df.values:  # iterate over the values\n",
    "    \n",
    "        prev_days.append([n for n in i[:-1]])  # store all but the target\n",
    "        if len(prev_days) == SEQ_LEN:  # make sure we have 10 sequences!\n",
    "            sequential_data.append([np.array(prev_days), i[-1]])  # i[-1] is the sequence target\n",
    "\n",
    "    \n",
    "    random.shuffle(sequential_data)  # shuffle for good measure.\n",
    "    \n",
    "    '''\n",
    "    # for classification we need this part of codes\n",
    "    buys = []  # list that will store our buy sequences and targets\n",
    "    sells = []  # list that will store our sell sequences and targets\n",
    "\n",
    "    for seq, target in sequential_data:  # iterate over the sequential data\n",
    "        if target == 0:  # if it's a \"not buy\"\n",
    "            sells.append([seq, target])  # append to sells list\n",
    "        elif target == 1:  # otherwise if the target is a 1...\n",
    "            buys.append([seq, target])  # it's a buy!\n",
    "\n",
    "    random.shuffle(buys)  # shuffle the buys\n",
    "    random.shuffle(sells)  # shuffle the sells!\n",
    "\n",
    "    lower = min(len(buys), len(sells))  # what's the shorter length?\n",
    "    \n",
    "    # for blancing dataset\n",
    "    buys = buys[:lower]  # make sure both lists are only up to the shortest length.\n",
    "    sells = sells[:lower]  # make sure both lists are only up to the shortest length.\n",
    "\n",
    "    sequential_data = buys+sells  # add them together\n",
    "    random.shuffle(sequential_data)  # another shuffle, so the model doesn't get confused with all 1 class then the other.\n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for seq, target in sequential_data:  # going over our new sequential data\n",
    "        X.append(seq)  # X is the sequences\n",
    "        y.append(target)  # y is the targets/labels (buys vs sell/notbuy)\n",
    "\n",
    "\n",
    "    \n",
    "    return np.array(X), y  # return X and y...and make X a numpy array!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(current, future):\n",
    "    if float(future) > float(current):  # if the future price is higher than the current, that's a buy, or a 1\n",
    "        return 1\n",
    "    else:  # otherwise... it's a 0!\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Open          High           Low         Close  \\\n",
      "timestamp                                                            \n",
      "2019-08-31   9597.539063   9673.220703   9531.799805   9630.664063   \n",
      "2019-09-01   9630.592773   9796.755859   9582.944336   9757.970703   \n",
      "2019-09-02   9757.473633  10396.591797   9730.650391  10346.760742   \n",
      "2019-09-03  10345.725586  10736.104492  10308.547852  10623.540039   \n",
      "2019-09-04  10621.180664  10762.644531  10434.709961  10594.493164   \n",
      "\n",
      "               Adj Close       Volume        target  \n",
      "timestamp                                            \n",
      "2019-08-31   9630.664063  11454806419   9757.970703  \n",
      "2019-09-01   9757.970703  11445355859  10346.760742  \n",
      "2019-09-02  10346.760742  17248102293  10623.540039  \n",
      "2019-09-03  10623.540039  19384917988  10594.493164  \n",
      "2019-09-04  10594.493164  16742664768  10575.533203  \n",
      "               Open          High           Low         Close     Adj Close  \\\n",
      "count    294.000000    294.000000    294.000000    294.000000    294.000000   \n",
      "mean    8438.538779   8602.269744   8264.934202   8437.378368   8437.378368   \n",
      "std     1234.745882   1211.192109   1253.558226   1234.707526   1234.707526   \n",
      "min     5002.578125   5331.833984   4106.980957   4970.788086   4970.788086   \n",
      "25%     7401.588257   7536.733765   7271.223633   7403.338745   7403.338745   \n",
      "50%     8598.533692   8745.209472   8384.715332   8598.169433   8598.169433   \n",
      "75%     9434.801270   9571.301514   9262.651612   9423.918701   9423.918701   \n",
      "max    10621.180664  10898.761719  10516.417969  10623.540039  10623.540039   \n",
      "\n",
      "             Volume        target  \n",
      "count  2.940000e+02    294.000000  \n",
      "mean   2.927721e+10   8436.363663  \n",
      "std    1.193265e+10   1233.845799  \n",
      "min    1.144536e+10   4970.788086  \n",
      "25%    1.933064e+10   7403.338745  \n",
      "50%    2.775058e+10   8598.169433  \n",
      "75%    3.764325e+10   9412.419434  \n",
      "max    7.415677e+10  10623.540039  \n"
     ]
    }
   ],
   "source": [
    "filename = 'BTC-USD.csv'\n",
    "dataset = filename\n",
    "main_df = pd.read_csv(dataset )  # read in specific file\n",
    "main_df['timestamp'] = main_df['Date'] #+' '+ main_df['time']\n",
    "\n",
    "main_df['timestamp'] = pd.to_datetime(main_df['timestamp'])\n",
    "main_df = main_df.sort_values('timestamp', axis = 0  )\n",
    "main_df = main_df.set_index('timestamp')\n",
    "main_df = main_df.drop('Date' ,1)\n",
    "#main_df  = main_df.drop('time',1)\n",
    "\n",
    "\n",
    "main_df.fillna(method=\"ffill\", inplace=True)  # if there are gaps in data, use previously known values\n",
    "main_df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# add target \n",
    "main_df['target'] = main_df['Close'].shift(-FUTURE_PERIOD_PREDICT)\n",
    "#main_df['target'] = list(map(classify, main_df['Close'], main_df['future']))\n",
    "\n",
    "\n",
    "#test and train seprate\n",
    "dates = sorted(main_df.index.values)  # get the dates\n",
    "last_5pct = sorted(main_df.index.values)[-int(0.2*len(dates))]  # get the last 20% of the times\n",
    "\n",
    "validation_main_df = main_df[(main_df.index >= last_5pct)]  # make the validation data where the index is in the last 5%\n",
    "main_df = main_df[(main_df.index < last_5pct)]  # now the main_df is all the data up to the last 5%\n",
    "print(main_df.head())\n",
    "print(main_df.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(285, 7, 3)\n",
      "train data: 285 validation: 63\n",
      "Dont buys: 0, buys: 0\n",
      "VALIDATION Dont buys: 0, buys: 0\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = preprocess_df(main_df)\n",
    "validation_x, validation_y = preprocess_df(validation_main_df)\n",
    "print(train_x.shape)\n",
    "print(f\"train data: {len(train_x)} validation: {len(validation_x)}\")\n",
    "print(f\"Dont buys: {train_y.count(0)}, buys: {train_y.count(1)}\")\n",
    "print(f\"VALIDATION Dont buys: {validation_y.count(0)}, buys: {validation_y.count(1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500  # how many passes through our data\n",
    "BATCH_SIZE = 64  # how many batches? Try smaller batch if you're getting OOM (out of memory) errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())  #normalizes activation outputs, same reason you want to normalize your input data.\n",
    "\n",
    "\n",
    "model.add(LSTM(64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(Dense(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 7, 128)            67584     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 7, 128)            512       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                49408     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 119,873\n",
      "Trainable params: 119,489\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.MeanAbsolutePercentageError(),\n",
    "    optimizer=opt,\n",
    "   \n",
    ")\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 285 samples, validate on 63 samples\n",
      "Epoch 1/500\n",
      "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002029B390DC8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x000002029B390DC8> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Index'\n",
      "285/285 [==============================] - 8s 30ms/sample - loss: 100.0077 - val_loss: 99.9996\n",
      "Epoch 2/500\n",
      "285/285 [==============================] - 0s 536us/sample - loss: 99.9948 - val_loss: 99.9992\n",
      "Epoch 3/500\n",
      "285/285 [==============================] - 0s 565us/sample - loss: 99.9893 - val_loss: 99.9986\n",
      "Epoch 4/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 99.9821 - val_loss: 99.9981\n",
      "Epoch 5/500\n",
      "285/285 [==============================] - 0s 575us/sample - loss: 99.9762 - val_loss: 99.9975\n",
      "Epoch 6/500\n",
      "285/285 [==============================] - 0s 603us/sample - loss: 99.9724 - val_loss: 99.9968\n",
      "Epoch 7/500\n",
      "285/285 [==============================] - 0s 554us/sample - loss: 99.9677 - val_loss: 99.9960\n",
      "Epoch 8/500\n",
      "285/285 [==============================] - 0s 572us/sample - loss: 99.9608 - val_loss: 99.9950\n",
      "Epoch 9/500\n",
      "285/285 [==============================] - 0s 694us/sample - loss: 99.9540 - val_loss: 99.9938\n",
      "Epoch 10/500\n",
      "285/285 [==============================] - 0s 638us/sample - loss: 99.9472 - val_loss: 99.9923\n",
      "Epoch 11/500\n",
      "285/285 [==============================] - 0s 614us/sample - loss: 99.9387 - val_loss: 99.9903\n",
      "Epoch 12/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 99.9315 - val_loss: 99.9878\n",
      "Epoch 13/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 99.9190 - val_loss: 99.9847\n",
      "Epoch 14/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 99.9108 - val_loss: 99.9810\n",
      "Epoch 15/500\n",
      "285/285 [==============================] - 0s 568us/sample - loss: 99.8981 - val_loss: 99.9765\n",
      "Epoch 16/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 99.8837 - val_loss: 99.9712\n",
      "Epoch 17/500\n",
      "285/285 [==============================] - 0s 565us/sample - loss: 99.8680 - val_loss: 99.9647\n",
      "Epoch 18/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 99.8565 - val_loss: 99.9573\n",
      "Epoch 19/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 99.8406 - val_loss: 99.9475\n",
      "Epoch 20/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 99.8275 - val_loss: 99.9359\n",
      "Epoch 21/500\n",
      "285/285 [==============================] - 0s 554us/sample - loss: 99.8067 - val_loss: 99.9238\n",
      "Epoch 22/500\n",
      "285/285 [==============================] - 0s 554us/sample - loss: 99.7866 - val_loss: 99.9086\n",
      "Epoch 23/500\n",
      "285/285 [==============================] - 0s 575us/sample - loss: 99.7632 - val_loss: 99.8972\n",
      "Epoch 24/500\n",
      "285/285 [==============================] - 0s 666us/sample - loss: 99.7357 - val_loss: 99.8826\n",
      "Epoch 25/500\n",
      "285/285 [==============================] - 0s 568us/sample - loss: 99.7223 - val_loss: 99.8667\n",
      "Epoch 26/500\n",
      "285/285 [==============================] - 0s 582us/sample - loss: 99.6999 - val_loss: 99.8491\n",
      "Epoch 27/500\n",
      "285/285 [==============================] - 0s 600us/sample - loss: 99.6800 - val_loss: 99.8287\n",
      "Epoch 28/500\n",
      "285/285 [==============================] - 0s 572us/sample - loss: 99.6442 - val_loss: 99.8056\n",
      "Epoch 29/500\n",
      "285/285 [==============================] - 0s 579us/sample - loss: 99.6146 - val_loss: 99.7777\n",
      "Epoch 30/500\n",
      "285/285 [==============================] - 0s 579us/sample - loss: 99.5947 - val_loss: 99.7426\n",
      "Epoch 31/500\n",
      "285/285 [==============================] - 0s 582us/sample - loss: 99.5610 - val_loss: 99.7141\n",
      "Epoch 32/500\n",
      "285/285 [==============================] - 0s 607us/sample - loss: 99.5316 - val_loss: 99.6850\n",
      "Epoch 33/500\n",
      "285/285 [==============================] - 0s 610us/sample - loss: 99.4992 - val_loss: 99.6583\n",
      "Epoch 34/500\n",
      "285/285 [==============================] - 0s 593us/sample - loss: 99.4572 - val_loss: 99.6311\n",
      "Epoch 35/500\n",
      "285/285 [==============================] - 0s 614us/sample - loss: 99.4246 - val_loss: 99.5939\n",
      "Epoch 36/500\n",
      "285/285 [==============================] - 0s 572us/sample - loss: 99.3885 - val_loss: 99.5594\n",
      "Epoch 37/500\n",
      "285/285 [==============================] - 0s 572us/sample - loss: 99.3387 - val_loss: 99.5209\n",
      "Epoch 38/500\n",
      "285/285 [==============================] - 0s 554us/sample - loss: 99.3100 - val_loss: 99.4774\n",
      "Epoch 39/500\n",
      "285/285 [==============================] - 0s 575us/sample - loss: 99.2474 - val_loss: 99.4337\n",
      "Epoch 40/500\n",
      "285/285 [==============================] - 0s 565us/sample - loss: 99.2041 - val_loss: 99.3852\n",
      "Epoch 41/500\n",
      "285/285 [==============================] - 0s 565us/sample - loss: 99.1809 - val_loss: 99.3282\n",
      "Epoch 42/500\n",
      "285/285 [==============================] - 0s 568us/sample - loss: 99.1226 - val_loss: 99.2778\n",
      "Epoch 43/500\n",
      "285/285 [==============================] - 0s 568us/sample - loss: 99.0621 - val_loss: 99.2289\n",
      "Epoch 44/500\n",
      "285/285 [==============================] - 0s 565us/sample - loss: 98.9985 - val_loss: 99.1656\n",
      "Epoch 45/500\n",
      "285/285 [==============================] - 0s 582us/sample - loss: 98.9462 - val_loss: 99.0873\n",
      "Epoch 46/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 98.8954 - val_loss: 99.0582\n",
      "Epoch 47/500\n",
      "285/285 [==============================] - 0s 659us/sample - loss: 98.8400 - val_loss: 98.9871\n",
      "Epoch 48/500\n",
      "285/285 [==============================] - 0s 614us/sample - loss: 98.7640 - val_loss: 98.9201\n",
      "Epoch 49/500\n",
      "285/285 [==============================] - 0s 565us/sample - loss: 98.7133 - val_loss: 98.9247\n",
      "Epoch 50/500\n",
      "285/285 [==============================] - 0s 565us/sample - loss: 98.6434 - val_loss: 98.8557\n",
      "Epoch 51/500\n",
      "285/285 [==============================] - 0s 603us/sample - loss: 98.5616 - val_loss: 98.7878\n",
      "Epoch 52/500\n",
      "285/285 [==============================] - 0s 638us/sample - loss: 98.5209 - val_loss: 98.6772\n",
      "Epoch 53/500\n",
      "285/285 [==============================] - 0s 610us/sample - loss: 98.4230 - val_loss: 98.5845\n",
      "Epoch 54/500\n",
      "285/285 [==============================] - 0s 621us/sample - loss: 98.3328 - val_loss: 98.5208\n",
      "Epoch 55/500\n",
      "285/285 [==============================] - 0s 575us/sample - loss: 98.2495 - val_loss: 98.4720\n",
      "Epoch 56/500\n",
      "285/285 [==============================] - 0s 575us/sample - loss: 98.1630 - val_loss: 98.4493\n",
      "Epoch 57/500\n",
      "285/285 [==============================] - 0s 568us/sample - loss: 98.1083 - val_loss: 98.4577\n",
      "Epoch 58/500\n",
      "285/285 [==============================] - 0s 572us/sample - loss: 98.0137 - val_loss: 98.4520\n",
      "Epoch 59/500\n",
      "285/285 [==============================] - 0s 582us/sample - loss: 97.9045 - val_loss: 98.3328\n",
      "Epoch 60/500\n",
      "285/285 [==============================] - 0s 607us/sample - loss: 97.8186 - val_loss: 98.2120\n",
      "Epoch 61/500\n",
      "285/285 [==============================] - 0s 579us/sample - loss: 97.7145 - val_loss: 98.1030\n",
      "Epoch 62/500\n",
      "285/285 [==============================] - 0s 579us/sample - loss: 97.6134 - val_loss: 98.1022\n",
      "Epoch 63/500\n",
      "285/285 [==============================] - 0s 586us/sample - loss: 97.4554 - val_loss: 98.0650\n",
      "Epoch 64/500\n",
      "285/285 [==============================] - 0s 586us/sample - loss: 97.4246 - val_loss: 98.0714\n",
      "Epoch 65/500\n",
      "285/285 [==============================] - 0s 589us/sample - loss: 97.2423 - val_loss: 98.1025\n",
      "Epoch 66/500\n",
      "285/285 [==============================] - 0s 586us/sample - loss: 97.2111 - val_loss: 98.0950\n",
      "Epoch 67/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 97.0349 - val_loss: 97.9481\n",
      "Epoch 68/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 96.9665 - val_loss: 97.7011\n",
      "Epoch 69/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285/285 [==============================] - 0s 614us/sample - loss: 96.8318 - val_loss: 97.4022\n",
      "Epoch 70/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 96.6322 - val_loss: 97.1611\n",
      "Epoch 71/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 96.5315 - val_loss: 96.8252\n",
      "Epoch 72/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 96.3342 - val_loss: 96.6133\n",
      "Epoch 73/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 96.2055 - val_loss: 96.5000\n",
      "Epoch 74/500\n",
      "285/285 [==============================] - 0s 515us/sample - loss: 96.0820 - val_loss: 96.3900\n",
      "Epoch 75/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 95.9480 - val_loss: 96.2822\n",
      "Epoch 76/500\n",
      "285/285 [==============================] - 0s 512us/sample - loss: 95.8050 - val_loss: 96.1879\n",
      "Epoch 77/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 95.6239 - val_loss: 96.1000\n",
      "Epoch 78/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 95.4576 - val_loss: 96.0476\n",
      "Epoch 79/500\n",
      "285/285 [==============================] - 0s 512us/sample - loss: 95.2209 - val_loss: 95.9047\n",
      "Epoch 80/500\n",
      "285/285 [==============================] - 0s 519us/sample - loss: 95.1124 - val_loss: 95.8631\n",
      "Epoch 81/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 94.8763 - val_loss: 95.8826\n",
      "Epoch 82/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 94.7571 - val_loss: 95.7555\n",
      "Epoch 83/500\n",
      "285/285 [==============================] - 0s 512us/sample - loss: 94.5624 - val_loss: 95.6697\n",
      "Epoch 84/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 94.3146 - val_loss: 95.4659\n",
      "Epoch 85/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 94.0778 - val_loss: 95.0710\n",
      "Epoch 86/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 93.9504 - val_loss: 94.8006\n",
      "Epoch 87/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 93.7490 - val_loss: 94.4846\n",
      "Epoch 88/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 93.4368 - val_loss: 94.5036\n",
      "Epoch 89/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 93.1095 - val_loss: 94.3966\n",
      "Epoch 90/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 92.9455 - val_loss: 94.3049\n",
      "Epoch 91/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 92.6139 - val_loss: 94.2731\n",
      "Epoch 92/500\n",
      "285/285 [==============================] - 0s 530us/sample - loss: 92.3861 - val_loss: 94.1159\n",
      "Epoch 93/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 92.3161 - val_loss: 93.9348\n",
      "Epoch 94/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 91.9937 - val_loss: 93.7853\n",
      "Epoch 95/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 91.6793 - val_loss: 93.2394\n",
      "Epoch 96/500\n",
      "285/285 [==============================] - 0s 515us/sample - loss: 91.3353 - val_loss: 92.9427\n",
      "Epoch 97/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 91.0459 - val_loss: 92.9805\n",
      "Epoch 98/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 90.5882 - val_loss: 93.0465\n",
      "Epoch 99/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 90.3176 - val_loss: 92.9529\n",
      "Epoch 100/500\n",
      "285/285 [==============================] - 0s 519us/sample - loss: 89.9447 - val_loss: 92.9238\n",
      "Epoch 101/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 89.9323 - val_loss: 92.6625\n",
      "Epoch 102/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 89.7460 - val_loss: 92.3677\n",
      "Epoch 103/500\n",
      "285/285 [==============================] - 0s 519us/sample - loss: 89.5100 - val_loss: 92.2336\n",
      "Epoch 104/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 89.2660 - val_loss: 91.9911\n",
      "Epoch 105/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 88.6821 - val_loss: 91.7380\n",
      "Epoch 106/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 88.2752 - val_loss: 91.2072\n",
      "Epoch 107/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 87.9730 - val_loss: 90.7488\n",
      "Epoch 108/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 87.6177 - val_loss: 90.7185\n",
      "Epoch 109/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 87.2501 - val_loss: 90.4394\n",
      "Epoch 110/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 86.6539 - val_loss: 90.0186\n",
      "Epoch 111/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 86.6321 - val_loss: 89.4953\n",
      "Epoch 112/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 85.5930 - val_loss: 89.3078\n",
      "Epoch 113/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 85.2122 - val_loss: 89.1097\n",
      "Epoch 114/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 85.1893 - val_loss: 88.7038\n",
      "Epoch 115/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 84.9151 - val_loss: 88.2146\n",
      "Epoch 116/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 84.1641 - val_loss: 87.7858\n",
      "Epoch 117/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 84.0035 - val_loss: 87.3994\n",
      "Epoch 118/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 82.9892 - val_loss: 87.1335\n",
      "Epoch 119/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 82.6728 - val_loss: 85.7969\n",
      "Epoch 120/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 82.5124 - val_loss: 84.9870\n",
      "Epoch 121/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 81.8895 - val_loss: 84.1235\n",
      "Epoch 122/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 81.5453 - val_loss: 84.1769\n",
      "Epoch 123/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 80.6561 - val_loss: 85.3815\n",
      "Epoch 124/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 79.8695 - val_loss: 84.3629\n",
      "Epoch 125/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 79.5713 - val_loss: 82.9346\n",
      "Epoch 126/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 79.1287 - val_loss: 80.6118\n",
      "Epoch 127/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 78.6151 - val_loss: 79.8141\n",
      "Epoch 128/500\n",
      "285/285 [==============================] - 0s 530us/sample - loss: 78.3268 - val_loss: 79.9728\n",
      "Epoch 129/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 77.8947 - val_loss: 79.8548\n",
      "Epoch 130/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 76.5876 - val_loss: 79.4331\n",
      "Epoch 131/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 76.0965 - val_loss: 79.0361\n",
      "Epoch 132/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 75.2546 - val_loss: 78.7742\n",
      "Epoch 133/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 74.9921 - val_loss: 79.3070\n",
      "Epoch 134/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 74.6469 - val_loss: 79.7683\n",
      "Epoch 135/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 74.6492 - val_loss: 77.4676\n",
      "Epoch 136/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 73.5489 - val_loss: 76.6425\n",
      "Epoch 137/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 72.3283 - val_loss: 73.8720\n",
      "Epoch 138/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 71.6162 - val_loss: 74.2657\n",
      "Epoch 139/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 70.7453 - val_loss: 71.9616\n",
      "Epoch 140/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 69.9490 - val_loss: 72.7422\n",
      "Epoch 141/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 68.9949 - val_loss: 73.7050\n",
      "Epoch 142/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 68.6951 - val_loss: 74.8048\n",
      "Epoch 143/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 68.2662 - val_loss: 71.5209\n",
      "Epoch 144/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285/285 [==============================] - 0s 544us/sample - loss: 67.5268 - val_loss: 72.6842\n",
      "Epoch 145/500\n",
      "285/285 [==============================] - 0s 522us/sample - loss: 68.0197 - val_loss: 71.0914\n",
      "Epoch 146/500\n",
      "285/285 [==============================] - 0s 621us/sample - loss: 66.2668 - val_loss: 68.4305\n",
      "Epoch 147/500\n",
      "285/285 [==============================] - 0s 656us/sample - loss: 65.7175 - val_loss: 67.3870\n",
      "Epoch 148/500\n",
      "285/285 [==============================] - 0s 645us/sample - loss: 64.1776 - val_loss: 68.6872\n",
      "Epoch 149/500\n",
      "285/285 [==============================] - 0s 614us/sample - loss: 64.6492 - val_loss: 69.8996\n",
      "Epoch 150/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 64.9051 - val_loss: 73.5431\n",
      "Epoch 151/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 66.6854 - val_loss: 75.8583\n",
      "Epoch 152/500\n",
      "285/285 [==============================] - 0s 617us/sample - loss: 67.2356 - val_loss: 73.1549\n",
      "Epoch 153/500\n",
      "285/285 [==============================] - 0s 603us/sample - loss: 64.0993 - val_loss: 68.9402\n",
      "Epoch 154/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 60.8660 - val_loss: 64.3904\n",
      "Epoch 155/500\n",
      "285/285 [==============================] - 0s 614us/sample - loss: 60.5165 - val_loss: 60.7598\n",
      "Epoch 156/500\n",
      "285/285 [==============================] - 0s 677us/sample - loss: 59.7858 - val_loss: 60.2616\n",
      "Epoch 157/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 59.5291 - val_loss: 60.8458\n",
      "Epoch 158/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 57.5564 - val_loss: 63.6005\n",
      "Epoch 159/500\n",
      "285/285 [==============================] - 0s 582us/sample - loss: 56.1500 - val_loss: 63.5510\n",
      "Epoch 160/500\n",
      "285/285 [==============================] - 0s 554us/sample - loss: 55.1959 - val_loss: 62.3583\n",
      "Epoch 161/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 52.5792 - val_loss: 63.6447\n",
      "Epoch 162/500\n",
      "285/285 [==============================] - 0s 568us/sample - loss: 52.5113 - val_loss: 64.5857\n",
      "Epoch 163/500\n",
      "285/285 [==============================] - 0s 554us/sample - loss: 50.5992 - val_loss: 63.3241\n",
      "Epoch 164/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 49.5214 - val_loss: 62.8504\n",
      "Epoch 165/500\n",
      "285/285 [==============================] - 0s 663us/sample - loss: 48.4495 - val_loss: 63.6015\n",
      "Epoch 166/500\n",
      "285/285 [==============================] - 0s 775us/sample - loss: 47.5622 - val_loss: 62.7934\n",
      "Epoch 167/500\n",
      "285/285 [==============================] - 0s 603us/sample - loss: 46.1516 - val_loss: 62.0048\n",
      "Epoch 168/500\n",
      "285/285 [==============================] - 0s 530us/sample - loss: 45.4835 - val_loss: 61.2421\n",
      "Epoch 169/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 42.9162 - val_loss: 60.6157\n",
      "Epoch 170/500\n",
      "285/285 [==============================] - 0s 565us/sample - loss: 43.6002 - val_loss: 59.1990\n",
      "Epoch 171/500\n",
      "285/285 [==============================] - 0s 554us/sample - loss: 41.4611 - val_loss: 58.4709\n",
      "Epoch 172/500\n",
      "285/285 [==============================] - 0s 568us/sample - loss: 41.2013 - val_loss: 58.1829\n",
      "Epoch 173/500\n",
      "285/285 [==============================] - 0s 628us/sample - loss: 40.5404 - val_loss: 55.0447\n",
      "Epoch 174/500\n",
      "285/285 [==============================] - 0s 642us/sample - loss: 38.8121 - val_loss: 53.4021\n",
      "Epoch 175/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 38.2480 - val_loss: 52.2498\n",
      "Epoch 176/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 37.9579 - val_loss: 50.8111\n",
      "Epoch 177/500\n",
      "285/285 [==============================] - 0s 572us/sample - loss: 37.7769 - val_loss: 50.0299\n",
      "Epoch 178/500\n",
      "285/285 [==============================] - 0s 554us/sample - loss: 35.5490 - val_loss: 49.3607\n",
      "Epoch 179/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 34.0312 - val_loss: 47.8634\n",
      "Epoch 180/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 31.4931 - val_loss: 49.2798\n",
      "Epoch 181/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 29.8387 - val_loss: 50.8736\n",
      "Epoch 182/500\n",
      "285/285 [==============================] - 0s 575us/sample - loss: 31.0012 - val_loss: 50.2253\n",
      "Epoch 183/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 31.5570 - val_loss: 46.0396\n",
      "Epoch 184/500\n",
      "285/285 [==============================] - 0s 565us/sample - loss: 29.8403 - val_loss: 43.6976\n",
      "Epoch 185/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 27.6441 - val_loss: 42.1061\n",
      "Epoch 186/500\n",
      "285/285 [==============================] - 0s 649us/sample - loss: 27.2030 - val_loss: 40.0666\n",
      "Epoch 187/500\n",
      "285/285 [==============================] - 0s 719us/sample - loss: 25.1404 - val_loss: 42.5199\n",
      "Epoch 188/500\n",
      "285/285 [==============================] - 0s 736us/sample - loss: 25.7664 - val_loss: 43.8152\n",
      "Epoch 189/500\n",
      "285/285 [==============================] - 0s 680us/sample - loss: 29.2257 - val_loss: 43.9771\n",
      "Epoch 190/500\n",
      "285/285 [==============================] - 0s 687us/sample - loss: 28.3729 - val_loss: 43.4592\n",
      "Epoch 191/500\n",
      "285/285 [==============================] - 0s 642us/sample - loss: 28.4231 - val_loss: 42.4473\n",
      "Epoch 192/500\n",
      "285/285 [==============================] - 0s 645us/sample - loss: 26.1569 - val_loss: 41.9563\n",
      "Epoch 193/500\n",
      "285/285 [==============================] - 0s 617us/sample - loss: 25.8058 - val_loss: 42.1058\n",
      "Epoch 194/500\n",
      "285/285 [==============================] - 0s 586us/sample - loss: 25.4065 - val_loss: 42.4005\n",
      "Epoch 195/500\n",
      "285/285 [==============================] - 0s 621us/sample - loss: 29.8220 - val_loss: 41.9864\n",
      "Epoch 196/500\n",
      "285/285 [==============================] - 0s 596us/sample - loss: 30.9616 - val_loss: 41.7369\n",
      "Epoch 197/500\n",
      "285/285 [==============================] - 0s 586us/sample - loss: 32.2443 - val_loss: 42.0549\n",
      "Epoch 198/500\n",
      "285/285 [==============================] - 0s 582us/sample - loss: 31.5672 - val_loss: 40.8543\n",
      "Epoch 199/500\n",
      "285/285 [==============================] - 0s 586us/sample - loss: 31.1880 - val_loss: 40.0736\n",
      "Epoch 200/500\n",
      "285/285 [==============================] - 0s 614us/sample - loss: 28.7959 - val_loss: 39.5227\n",
      "Epoch 201/500\n",
      "285/285 [==============================] - 0s 687us/sample - loss: 27.0267 - val_loss: 39.5079\n",
      "Epoch 202/500\n",
      "285/285 [==============================] - 0s 586us/sample - loss: 25.9617 - val_loss: 38.9210\n",
      "Epoch 203/500\n",
      "285/285 [==============================] - 0s 568us/sample - loss: 24.6508 - val_loss: 37.0645\n",
      "Epoch 204/500\n",
      "285/285 [==============================] - 0s 771us/sample - loss: 22.9654 - val_loss: 35.2924\n",
      "Epoch 205/500\n",
      "285/285 [==============================] - 0s 649us/sample - loss: 22.4015 - val_loss: 35.4050\n",
      "Epoch 206/500\n",
      "285/285 [==============================] - 0s 740us/sample - loss: 21.7970 - val_loss: 35.3264\n",
      "Epoch 207/500\n",
      "285/285 [==============================] - 0s 617us/sample - loss: 22.3247 - val_loss: 34.7745\n",
      "Epoch 208/500\n",
      "285/285 [==============================] - 0s 572us/sample - loss: 20.3916 - val_loss: 34.2245\n",
      "Epoch 209/500\n",
      "285/285 [==============================] - 0s 582us/sample - loss: 20.6800 - val_loss: 34.5301\n",
      "Epoch 210/500\n",
      "285/285 [==============================] - 0s 582us/sample - loss: 20.6357 - val_loss: 33.8587\n",
      "Epoch 211/500\n",
      "285/285 [==============================] - 0s 593us/sample - loss: 20.5352 - val_loss: 33.9402\n",
      "Epoch 212/500\n",
      "285/285 [==============================] - 0s 575us/sample - loss: 21.2677 - val_loss: 33.4671\n",
      "Epoch 213/500\n",
      "285/285 [==============================] - 0s 624us/sample - loss: 19.4177 - val_loss: 33.1215\n",
      "Epoch 214/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 21.5193 - val_loss: 32.7310\n",
      "Epoch 215/500\n",
      "285/285 [==============================] - 0s 593us/sample - loss: 21.1913 - val_loss: 32.0630\n",
      "Epoch 216/500\n",
      "285/285 [==============================] - 0s 586us/sample - loss: 20.9275 - val_loss: 30.5015\n",
      "Epoch 217/500\n",
      "285/285 [==============================] - ETA: 0s - loss: 18.82 - 0s 635us/sample - loss: 20.2373 - val_loss: 29.6038\n",
      "Epoch 218/500\n",
      "285/285 [==============================] - 0s 600us/sample - loss: 19.1515 - val_loss: 29.5069\n",
      "Epoch 219/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285/285 [==============================] - 0s 586us/sample - loss: 18.4131 - val_loss: 28.4803\n",
      "Epoch 220/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 19.3237 - val_loss: 28.1640\n",
      "Epoch 221/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 19.3778 - val_loss: 28.1327\n",
      "Epoch 222/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 19.0540 - val_loss: 27.1267\n",
      "Epoch 223/500\n",
      "285/285 [==============================] - 0s 530us/sample - loss: 16.4592 - val_loss: 26.2848\n",
      "Epoch 224/500\n",
      "285/285 [==============================] - 0s 515us/sample - loss: 19.5950 - val_loss: 25.9643\n",
      "Epoch 225/500\n",
      "285/285 [==============================] - 0s 519us/sample - loss: 17.8453 - val_loss: 25.7696\n",
      "Epoch 226/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 19.0134 - val_loss: 26.6649\n",
      "Epoch 227/500\n",
      "285/285 [==============================] - 0s 522us/sample - loss: 20.3257 - val_loss: 26.1460\n",
      "Epoch 228/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 18.5126 - val_loss: 25.8598\n",
      "Epoch 229/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 17.6788 - val_loss: 25.6550\n",
      "Epoch 230/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 19.3328 - val_loss: 26.6502\n",
      "Epoch 231/500\n",
      "285/285 [==============================] - 0s 522us/sample - loss: 19.5226 - val_loss: 26.6121\n",
      "Epoch 232/500\n",
      "285/285 [==============================] - 0s 530us/sample - loss: 18.2938 - val_loss: 26.4958\n",
      "Epoch 233/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 18.8023 - val_loss: 25.3184\n",
      "Epoch 234/500\n",
      "285/285 [==============================] - 0s 522us/sample - loss: 17.7660 - val_loss: 25.0714\n",
      "Epoch 235/500\n",
      "285/285 [==============================] - 0s 522us/sample - loss: 17.1277 - val_loss: 24.6281\n",
      "Epoch 236/500\n",
      "285/285 [==============================] - 0s 530us/sample - loss: 17.2427 - val_loss: 24.2820\n",
      "Epoch 237/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 16.8954 - val_loss: 23.9268\n",
      "Epoch 238/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 18.1084 - val_loss: 23.8714\n",
      "Epoch 239/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 18.2382 - val_loss: 24.2605\n",
      "Epoch 240/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 17.8817 - val_loss: 24.6046\n",
      "Epoch 241/500\n",
      "285/285 [==============================] - 0s 519us/sample - loss: 18.3033 - val_loss: 27.3453\n",
      "Epoch 242/500\n",
      "285/285 [==============================] - 0s 530us/sample - loss: 18.4284 - val_loss: 25.6712\n",
      "Epoch 243/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 19.9452 - val_loss: 25.5923\n",
      "Epoch 244/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 19.6329 - val_loss: 25.5053\n",
      "Epoch 245/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 19.1136 - val_loss: 24.0501\n",
      "Epoch 246/500\n",
      "285/285 [==============================] - 0s 554us/sample - loss: 18.7428 - val_loss: 23.4228\n",
      "Epoch 247/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 18.0721 - val_loss: 23.5401\n",
      "Epoch 248/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 18.9626 - val_loss: 23.2473\n",
      "Epoch 249/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 18.5909 - val_loss: 23.4808\n",
      "Epoch 250/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 17.1711 - val_loss: 23.5853\n",
      "Epoch 251/500\n",
      "285/285 [==============================] - 0s 554us/sample - loss: 18.3146 - val_loss: 23.3374\n",
      "Epoch 252/500\n",
      "285/285 [==============================] - 0s 747us/sample - loss: 18.3498 - val_loss: 23.0436\n",
      "Epoch 253/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 17.1231 - val_loss: 22.3944\n",
      "Epoch 254/500\n",
      "285/285 [==============================] - ETA: 0s - loss: 19.43 - 0s 750us/sample - loss: 19.2054 - val_loss: 21.8695\n",
      "Epoch 255/500\n",
      "285/285 [==============================] - 0s 530us/sample - loss: 20.6730 - val_loss: 21.7377\n",
      "Epoch 256/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 19.8382 - val_loss: 20.7492\n",
      "Epoch 257/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 20.0708 - val_loss: 20.2019\n",
      "Epoch 258/500\n",
      "285/285 [==============================] - 0s 522us/sample - loss: 21.8693 - val_loss: 19.1756\n",
      "Epoch 259/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 18.8591 - val_loss: 18.8264\n",
      "Epoch 260/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 19.9501 - val_loss: 18.6094\n",
      "Epoch 261/500\n",
      "285/285 [==============================] - 0s 554us/sample - loss: 19.7587 - val_loss: 18.4774\n",
      "Epoch 262/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 19.6017 - val_loss: 18.1109\n",
      "Epoch 263/500\n",
      "285/285 [==============================] - 0s 522us/sample - loss: 19.6207 - val_loss: 17.6858\n",
      "Epoch 264/500\n",
      "285/285 [==============================] - 0s 515us/sample - loss: 18.5959 - val_loss: 17.6274\n",
      "Epoch 265/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 19.7326 - val_loss: 18.9745\n",
      "Epoch 266/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 17.4705 - val_loss: 19.3545\n",
      "Epoch 267/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 18.9107 - val_loss: 20.2572\n",
      "Epoch 268/500\n",
      "285/285 [==============================] - 0s 593us/sample - loss: 19.7203 - val_loss: 20.4238\n",
      "Epoch 269/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 20.3788 - val_loss: 20.6900\n",
      "Epoch 270/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 21.2901 - val_loss: 20.8184\n",
      "Epoch 271/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 22.0805 - val_loss: 21.1774\n",
      "Epoch 272/500\n",
      "285/285 [==============================] - 0s 554us/sample - loss: 19.6747 - val_loss: 20.4666\n",
      "Epoch 273/500\n",
      "285/285 [==============================] - 0s 554us/sample - loss: 20.2335 - val_loss: 20.3355\n",
      "Epoch 274/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 16.9812 - val_loss: 20.3370\n",
      "Epoch 275/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 17.3586 - val_loss: 20.4301\n",
      "Epoch 276/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 18.9784 - val_loss: 20.5882\n",
      "Epoch 277/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 20.2397 - val_loss: 20.9291\n",
      "Epoch 278/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 19.1070 - val_loss: 21.2411\n",
      "Epoch 279/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 21.1177 - val_loss: 21.3935\n",
      "Epoch 280/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 19.2506 - val_loss: 21.3463\n",
      "Epoch 281/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 20.7694 - val_loss: 21.1561\n",
      "Epoch 282/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 19.5029 - val_loss: 20.9404\n",
      "Epoch 283/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 20.4109 - val_loss: 20.6243\n",
      "Epoch 284/500\n",
      "285/285 [==============================] - 0s 568us/sample - loss: 21.1588 - val_loss: 20.4222\n",
      "Epoch 285/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 19.9362 - val_loss: 20.5017\n",
      "Epoch 286/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 19.0430 - val_loss: 20.7372\n",
      "Epoch 287/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 19.6281 - val_loss: 21.5726\n",
      "Epoch 288/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 18.0940 - val_loss: 22.5984\n",
      "Epoch 289/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 21.8007 - val_loss: 23.5103\n",
      "Epoch 290/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 22.5337 - val_loss: 23.8800\n",
      "Epoch 291/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 24.3745 - val_loss: 25.4104\n",
      "Epoch 292/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 29.8145 - val_loss: 26.2598\n",
      "Epoch 293/500\n",
      "285/285 [==============================] - 0s 565us/sample - loss: 28.8893 - val_loss: 26.8703\n",
      "Epoch 294/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285/285 [==============================] - 0s 540us/sample - loss: 28.2541 - val_loss: 26.9070\n",
      "Epoch 295/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 27.3083 - val_loss: 27.2188\n",
      "Epoch 296/500\n",
      "285/285 [==============================] - 0s 512us/sample - loss: 26.3217 - val_loss: 27.3751\n",
      "Epoch 297/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 23.2385 - val_loss: 27.7021\n",
      "Epoch 298/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 22.5498 - val_loss: 28.0213\n",
      "Epoch 299/500\n",
      "285/285 [==============================] - 0s 522us/sample - loss: 20.7139 - val_loss: 28.3460\n",
      "Epoch 300/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 19.2177 - val_loss: 28.8333\n",
      "Epoch 301/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 18.9364 - val_loss: 27.2296\n",
      "Epoch 302/500\n",
      "285/285 [==============================] - 0s 519us/sample - loss: 18.8655 - val_loss: 26.8276\n",
      "Epoch 303/500\n",
      "285/285 [==============================] - 0s 522us/sample - loss: 17.6494 - val_loss: 25.4523\n",
      "Epoch 304/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 18.9963 - val_loss: 25.7568\n",
      "Epoch 305/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 18.9900 - val_loss: 26.2382\n",
      "Epoch 306/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 19.3363 - val_loss: 25.5750\n",
      "Epoch 307/500\n",
      "285/285 [==============================] - 0s 515us/sample - loss: 19.9548 - val_loss: 24.9078\n",
      "Epoch 308/500\n",
      "285/285 [==============================] - 0s 530us/sample - loss: 17.2263 - val_loss: 24.4585\n",
      "Epoch 309/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 18.3977 - val_loss: 24.4696\n",
      "Epoch 310/500\n",
      "285/285 [==============================] - 0s 530us/sample - loss: 18.6278 - val_loss: 24.7606\n",
      "Epoch 311/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 20.0883 - val_loss: 25.8478\n",
      "Epoch 312/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 21.7212 - val_loss: 25.3082\n",
      "Epoch 313/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 20.0959 - val_loss: 24.6195\n",
      "Epoch 314/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 20.2121 - val_loss: 24.3023\n",
      "Epoch 315/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 22.1786 - val_loss: 23.4785\n",
      "Epoch 316/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 18.2201 - val_loss: 25.0361\n",
      "Epoch 317/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 19.9940 - val_loss: 24.6445\n",
      "Epoch 318/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 18.6758 - val_loss: 22.7930\n",
      "Epoch 319/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 18.3728 - val_loss: 22.4322\n",
      "Epoch 320/500\n",
      "285/285 [==============================] - 0s 596us/sample - loss: 18.8279 - val_loss: 22.2012\n",
      "Epoch 321/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 19.6213 - val_loss: 21.8824\n",
      "Epoch 322/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 18.2949 - val_loss: 21.4958\n",
      "Epoch 323/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 18.1915 - val_loss: 21.1249\n",
      "Epoch 324/500\n",
      "285/285 [==============================] - 0s 519us/sample - loss: 17.3573 - val_loss: 20.7149\n",
      "Epoch 325/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 16.9835 - val_loss: 20.4557\n",
      "Epoch 326/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 15.6194 - val_loss: 20.4334\n",
      "Epoch 327/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 16.2202 - val_loss: 20.3589\n",
      "Epoch 328/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 16.8567 - val_loss: 20.5075\n",
      "Epoch 329/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 17.0041 - val_loss: 20.7438\n",
      "Epoch 330/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 20.3904 - val_loss: 20.8038\n",
      "Epoch 331/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 17.3708 - val_loss: 20.9725\n",
      "Epoch 332/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 16.0303 - val_loss: 21.0658\n",
      "Epoch 333/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 17.9725 - val_loss: 21.3609\n",
      "Epoch 334/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 17.9242 - val_loss: 21.7263\n",
      "Epoch 335/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 20.3537 - val_loss: 21.7156\n",
      "Epoch 336/500\n",
      "285/285 [==============================] - 0s 575us/sample - loss: 17.6616 - val_loss: 21.5331\n",
      "Epoch 337/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 16.6258 - val_loss: 21.2401\n",
      "Epoch 338/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 20.0123 - val_loss: 21.9118\n",
      "Epoch 339/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 19.0658 - val_loss: 20.9150\n",
      "Epoch 340/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 19.7817 - val_loss: 20.7621\n",
      "Epoch 341/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 18.3050 - val_loss: 20.5993\n",
      "Epoch 342/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 18.8634 - val_loss: 20.4572\n",
      "Epoch 343/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 20.4416 - val_loss: 20.1635\n",
      "Epoch 344/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 18.0181 - val_loss: 19.9762\n",
      "Epoch 345/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 18.3648 - val_loss: 20.0041\n",
      "Epoch 346/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 18.6977 - val_loss: 19.9571\n",
      "Epoch 347/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 18.0353 - val_loss: 19.8859\n",
      "Epoch 348/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 19.4201 - val_loss: 19.7836\n",
      "Epoch 349/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 17.9845 - val_loss: 19.7774\n",
      "Epoch 350/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 17.3659 - val_loss: 19.8458\n",
      "Epoch 351/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 17.7129 - val_loss: 19.8923\n",
      "Epoch 352/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 18.5556 - val_loss: 20.0076\n",
      "Epoch 353/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 17.3919 - val_loss: 19.9978\n",
      "Epoch 354/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 18.6580 - val_loss: 19.7379\n",
      "Epoch 355/500\n",
      "285/285 [==============================] - 0s 565us/sample - loss: 16.7618 - val_loss: 19.9008\n",
      "Epoch 356/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 15.9558 - val_loss: 20.0567\n",
      "Epoch 357/500\n",
      "285/285 [==============================] - 0s 554us/sample - loss: 17.5497 - val_loss: 20.2222\n",
      "Epoch 358/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 16.4475 - val_loss: 20.4981\n",
      "Epoch 359/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 16.8007 - val_loss: 20.8534\n",
      "Epoch 360/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 16.3124 - val_loss: 20.7079\n",
      "Epoch 361/500\n",
      "285/285 [==============================] - 0s 554us/sample - loss: 16.3048 - val_loss: 20.8292\n",
      "Epoch 362/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 16.1147 - val_loss: 20.8919\n",
      "Epoch 363/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 20.8317 - val_loss: 21.0213\n",
      "Epoch 364/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 16.9192 - val_loss: 21.2273\n",
      "Epoch 365/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 15.9073 - val_loss: 21.4544\n",
      "Epoch 366/500\n",
      "285/285 [==============================] - 0s 554us/sample - loss: 16.9665 - val_loss: 21.6626\n",
      "Epoch 367/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 18.0204 - val_loss: 21.6831\n",
      "Epoch 368/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 16.8308 - val_loss: 21.9712\n",
      "Epoch 369/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285/285 [==============================] - 0s 551us/sample - loss: 16.8440 - val_loss: 22.4607\n",
      "Epoch 370/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 17.7821 - val_loss: 22.5957\n",
      "Epoch 371/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 16.9417 - val_loss: 22.4551\n",
      "Epoch 372/500\n",
      "285/285 [==============================] - 0s 515us/sample - loss: 19.2937 - val_loss: 22.5529\n",
      "Epoch 373/500\n",
      "285/285 [==============================] - 0s 522us/sample - loss: 20.7724 - val_loss: 22.8066\n",
      "Epoch 374/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 21.3101 - val_loss: 23.0352\n",
      "Epoch 375/500\n",
      "285/285 [==============================] - 0s 515us/sample - loss: 19.2127 - val_loss: 23.0042\n",
      "Epoch 376/500\n",
      "285/285 [==============================] - 0s 522us/sample - loss: 18.8101 - val_loss: 22.9991\n",
      "Epoch 377/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 20.6797 - val_loss: 23.1061\n",
      "Epoch 378/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 19.5844 - val_loss: 24.4729\n",
      "Epoch 379/500\n",
      "285/285 [==============================] - 0s 519us/sample - loss: 18.3049 - val_loss: 23.4033\n",
      "Epoch 380/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 17.7130 - val_loss: 23.5039\n",
      "Epoch 381/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 17.6805 - val_loss: 23.4885\n",
      "Epoch 382/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 16.8423 - val_loss: 23.3755\n",
      "Epoch 383/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 16.6831 - val_loss: 23.2227\n",
      "Epoch 384/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 16.8167 - val_loss: 23.1047\n",
      "Epoch 385/500\n",
      "285/285 [==============================] - 0s 530us/sample - loss: 16.7711 - val_loss: 23.1328\n",
      "Epoch 386/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 17.4448 - val_loss: 23.1330\n",
      "Epoch 387/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 16.1567 - val_loss: 23.0497\n",
      "Epoch 388/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 16.9662 - val_loss: 23.2067\n",
      "Epoch 389/500\n",
      "285/285 [==============================] - 0s 522us/sample - loss: 17.0446 - val_loss: 23.3170\n",
      "Epoch 390/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 19.2307 - val_loss: 23.2616\n",
      "Epoch 391/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 19.3786 - val_loss: 23.6511\n",
      "Epoch 392/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 19.2821 - val_loss: 23.0896\n",
      "Epoch 393/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 19.4179 - val_loss: 23.2277\n",
      "Epoch 394/500\n",
      "285/285 [==============================] - 0s 673us/sample - loss: 19.2263 - val_loss: 23.3531\n",
      "Epoch 395/500\n",
      "285/285 [==============================] - 0s 568us/sample - loss: 21.7276 - val_loss: 23.6008\n",
      "Epoch 396/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 20.5413 - val_loss: 23.7477\n",
      "Epoch 397/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 18.5859 - val_loss: 23.8548\n",
      "Epoch 398/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 20.0975 - val_loss: 23.8680\n",
      "Epoch 399/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 19.7384 - val_loss: 23.9202\n",
      "Epoch 400/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 20.3120 - val_loss: 24.0325\n",
      "Epoch 401/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 18.8341 - val_loss: 23.8754\n",
      "Epoch 402/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 17.8297 - val_loss: 23.3425\n",
      "Epoch 403/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 18.5471 - val_loss: 23.0838\n",
      "Epoch 404/500\n",
      "285/285 [==============================] - 0s 519us/sample - loss: 18.4727 - val_loss: 22.9770\n",
      "Epoch 405/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 15.9817 - val_loss: 22.9429\n",
      "Epoch 406/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 16.9266 - val_loss: 22.8856\n",
      "Epoch 407/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 15.8161 - val_loss: 22.8121\n",
      "Epoch 408/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 17.4713 - val_loss: 22.7502\n",
      "Epoch 409/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 17.5039 - val_loss: 22.7147\n",
      "Epoch 410/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 16.7626 - val_loss: 22.7857\n",
      "Epoch 411/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 16.1889 - val_loss: 22.8371\n",
      "Epoch 412/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 17.7628 - val_loss: 23.1665\n",
      "Epoch 413/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 18.7736 - val_loss: 22.9957\n",
      "Epoch 414/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 15.2854 - val_loss: 22.8169\n",
      "Epoch 415/500\n",
      "285/285 [==============================] - 0s 565us/sample - loss: 17.2849 - val_loss: 22.6702\n",
      "Epoch 416/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 16.7300 - val_loss: 22.6816\n",
      "Epoch 417/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 15.4749 - val_loss: 22.6326\n",
      "Epoch 418/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 18.1161 - val_loss: 22.6472\n",
      "Epoch 419/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 17.3893 - val_loss: 22.5696\n",
      "Epoch 420/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 16.1464 - val_loss: 22.6122\n",
      "Epoch 421/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 16.0553 - val_loss: 22.7523\n",
      "Epoch 422/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 17.4602 - val_loss: 22.4766\n",
      "Epoch 423/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 19.2842 - val_loss: 21.8716\n",
      "Epoch 424/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 19.6598 - val_loss: 21.5353\n",
      "Epoch 425/500\n",
      "285/285 [==============================] - 0s 565us/sample - loss: 21.9395 - val_loss: 21.2513\n",
      "Epoch 426/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 24.0126 - val_loss: 21.0884\n",
      "Epoch 427/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 21.7602 - val_loss: 20.5529\n",
      "Epoch 428/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 21.9230 - val_loss: 20.0080\n",
      "Epoch 429/500\n",
      "285/285 [==============================] - 0s 554us/sample - loss: 20.1028 - val_loss: 19.7036\n",
      "Epoch 430/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 20.6183 - val_loss: 19.5709\n",
      "Epoch 431/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 19.0750 - val_loss: 20.1098\n",
      "Epoch 432/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 18.2255 - val_loss: 19.3947\n",
      "Epoch 433/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 17.9005 - val_loss: 19.2846\n",
      "Epoch 434/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 18.8566 - val_loss: 19.1283\n",
      "Epoch 435/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 16.5311 - val_loss: 19.2242\n",
      "Epoch 436/500\n",
      "285/285 [==============================] - 0s 547us/sample - loss: 19.1994 - val_loss: 19.3222\n",
      "Epoch 437/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 16.3108 - val_loss: 19.5402\n",
      "Epoch 438/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 17.2195 - val_loss: 19.8197\n",
      "Epoch 439/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 16.6293 - val_loss: 20.1085\n",
      "Epoch 440/500\n",
      "285/285 [==============================] - 0s 558us/sample - loss: 18.0687 - val_loss: 20.1625\n",
      "Epoch 441/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 17.7863 - val_loss: 20.3069\n",
      "Epoch 442/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 17.9134 - val_loss: 21.7367\n",
      "Epoch 443/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 18.9833 - val_loss: 20.8262\n",
      "Epoch 444/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285/285 [==============================] - 0s 568us/sample - loss: 17.6141 - val_loss: 20.8593\n",
      "Epoch 445/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 18.3985 - val_loss: 21.1297\n",
      "Epoch 446/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 17.9213 - val_loss: 21.0878\n",
      "Epoch 447/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 16.9064 - val_loss: 21.2197\n",
      "Epoch 448/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 19.5111 - val_loss: 21.4267\n",
      "Epoch 449/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 20.5848 - val_loss: 21.6002\n",
      "Epoch 450/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 20.2613 - val_loss: 21.6992\n",
      "Epoch 451/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 16.4546 - val_loss: 21.8210\n",
      "Epoch 452/500\n",
      "285/285 [==============================] - 0s 526us/sample - loss: 18.4177 - val_loss: 22.0746\n",
      "Epoch 453/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 16.2312 - val_loss: 22.3391\n",
      "Epoch 454/500\n",
      "285/285 [==============================] - 0s 522us/sample - loss: 16.0508 - val_loss: 22.8925\n",
      "Epoch 455/500\n",
      "285/285 [==============================] - 0s 512us/sample - loss: 18.4460 - val_loss: 22.4441\n",
      "Epoch 456/500\n",
      "285/285 [==============================] - 0s 522us/sample - loss: 17.3092 - val_loss: 22.6537\n",
      "Epoch 457/500\n",
      "285/285 [==============================] - 0s 533us/sample - loss: 16.8339 - val_loss: 22.7697\n",
      "Epoch 458/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 16.2479 - val_loss: 22.7708\n",
      "Epoch 459/500\n",
      "285/285 [==============================] - 0s 530us/sample - loss: 18.7688 - val_loss: 22.6778\n",
      "Epoch 460/500\n",
      "285/285 [==============================] - 0s 544us/sample - loss: 18.8200 - val_loss: 22.6332\n",
      "Epoch 461/500\n",
      "285/285 [==============================] - 0s 530us/sample - loss: 17.1795 - val_loss: 22.5233\n",
      "Epoch 462/500\n",
      "285/285 [==============================] - 0s 537us/sample - loss: 18.0767 - val_loss: 22.2418\n",
      "Epoch 463/500\n",
      "285/285 [==============================] - 0s 519us/sample - loss: 16.1124 - val_loss: 22.0891\n",
      "Epoch 464/500\n",
      "285/285 [==============================] - 0s 575us/sample - loss: 17.1336 - val_loss: 22.1502\n",
      "Epoch 465/500\n",
      "285/285 [==============================] - 0s 568us/sample - loss: 15.4672 - val_loss: 22.0100\n",
      "Epoch 466/500\n",
      "285/285 [==============================] - 0s 568us/sample - loss: 19.8462 - val_loss: 21.7714\n",
      "Epoch 467/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 17.6427 - val_loss: 21.5481\n",
      "Epoch 468/500\n",
      "285/285 [==============================] - 0s 554us/sample - loss: 15.4835 - val_loss: 21.4324\n",
      "Epoch 469/500\n",
      "285/285 [==============================] - 0s 551us/sample - loss: 17.7558 - val_loss: 21.0620\n",
      "Epoch 470/500\n",
      "285/285 [==============================] - 0s 561us/sample - loss: 17.2908 - val_loss: 20.7201\n",
      "Epoch 471/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 17.4907 - val_loss: 20.8680\n",
      "Epoch 472/500\n",
      "285/285 [==============================] - 0s 529us/sample - loss: 15.6769 - val_loss: 21.3246\n",
      "Epoch 473/500\n",
      "285/285 [==============================] - 0s 540us/sample - loss: 17.3429 - val_loss: 21.6436\n",
      "Epoch 474/500\n",
      "285/285 [==============================] - 0s 635us/sample - loss: 16.7416 - val_loss: 21.4000\n",
      "Epoch 475/500\n",
      "285/285 [==============================] - 0s 684us/sample - loss: 17.7303 - val_loss: 21.2140\n",
      "Epoch 476/500\n",
      "285/285 [==============================] - 0s 733us/sample - loss: 17.5115 - val_loss: 21.1291\n",
      "Epoch 477/500\n",
      "285/285 [==============================] - 0s 670us/sample - loss: 19.2758 - val_loss: 20.8607\n",
      "Epoch 478/500\n",
      "285/285 [==============================] - 0s 694us/sample - loss: 15.8168 - val_loss: 21.0659\n",
      "Epoch 479/500\n",
      "285/285 [==============================] - 0s 638us/sample - loss: 17.4934 - val_loss: 21.3323\n",
      "Epoch 480/500\n",
      "285/285 [==============================] - 0s 610us/sample - loss: 18.4669 - val_loss: 21.3973\n",
      "Epoch 481/500\n",
      "285/285 [==============================] - ETA: 0s - loss: 18.69 - 0s 607us/sample - loss: 17.8817 - val_loss: 21.6386\n",
      "Epoch 482/500\n",
      "285/285 [==============================] - 0s 589us/sample - loss: 15.6559 - val_loss: 21.9709\n",
      "Epoch 483/500\n",
      "285/285 [==============================] - 0s 586us/sample - loss: 16.6585 - val_loss: 22.1488\n",
      "Epoch 484/500\n",
      "285/285 [==============================] - 0s 596us/sample - loss: 17.3632 - val_loss: 22.4228\n",
      "Epoch 485/500\n",
      "285/285 [==============================] - 0s 635us/sample - loss: 16.4616 - val_loss: 23.6746\n",
      "Epoch 486/500\n",
      "285/285 [==============================] - 0s 628us/sample - loss: 17.1684 - val_loss: 24.1127\n",
      "Epoch 487/500\n",
      "285/285 [==============================] - 0s 600us/sample - loss: 17.2806 - val_loss: 24.1688\n",
      "Epoch 488/500\n",
      "285/285 [==============================] - 0s 593us/sample - loss: 18.6198 - val_loss: 22.6172\n",
      "Epoch 489/500\n",
      "285/285 [==============================] - 0s 593us/sample - loss: 16.0627 - val_loss: 20.6330\n",
      "Epoch 490/500\n",
      "285/285 [==============================] - 0s 600us/sample - loss: 19.4978 - val_loss: 19.7416\n",
      "Epoch 491/500\n",
      "285/285 [==============================] - 0s 579us/sample - loss: 16.7691 - val_loss: 19.4773\n",
      "Epoch 492/500\n",
      "285/285 [==============================] - 0s 603us/sample - loss: 18.9410 - val_loss: 19.4908\n",
      "Epoch 493/500\n",
      "285/285 [==============================] - 0s 708us/sample - loss: 16.5409 - val_loss: 19.9437\n",
      "Epoch 494/500\n",
      "285/285 [==============================] - 0s 610us/sample - loss: 18.3889 - val_loss: 20.5446\n",
      "Epoch 495/500\n",
      "285/285 [==============================] - 0s 642us/sample - loss: 18.3556 - val_loss: 21.3407\n",
      "Epoch 496/500\n",
      "285/285 [==============================] - 0s 610us/sample - loss: 19.3800 - val_loss: 21.6076\n",
      "Epoch 497/500\n",
      "285/285 [==============================] - 0s 628us/sample - loss: 19.1311 - val_loss: 22.2849\n",
      "Epoch 498/500\n",
      "285/285 [==============================] - 0s 579us/sample - loss: 18.3488 - val_loss: 22.7769\n",
      "Epoch 499/500\n",
      "285/285 [==============================] - 0s 593us/sample - loss: 19.2258 - val_loss: 22.8845\n",
      "Epoch 500/500\n",
      "285/285 [==============================] - 0s 596us/sample - loss: 18.9599 - val_loss: 22.8164\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_x, np.array(train_y),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(validation_x, np.array(validation_y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  #plt.ylim([0, 10])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Absolute Percentage Error [Close]')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "\n",
    "\n",
    "plot_loss(history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
